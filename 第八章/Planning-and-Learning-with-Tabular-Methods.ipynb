{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 第8章：Planning and Learning with Tabular Methods （表格规划与学习法）\n",
    "\n",
    "第8章探讨了强化学习中\"规划\"（planning）和\"学习\"（learning）之间的关系，重点介绍如何在表格方法（tabular methods）下结合两者的优势。这一章将前几章的无模型学习（如TD和蒙特卡洛）与基于模型的规划（如动态规划）统一起来，主要提出了**Dyna**架构作为关键创新。\n",
    "\n",
    "但实际上于今日（2025年）的强化学习中，\"规划\"和\"学习\"的界限已经模糊，而且框架也不再局限于表格方法。现代强化学习（如深度强化学习）通常使用函数逼近（neural network）来处理大规模状态空间。\n",
    "\n",
    "### 8.1 背景与核心问题\n",
    "- **学习与规划的区别**：\n",
    "  - **学习（Learning）**：从与环境的真实交互中获取经验，更新价值估计或策略（如第6章的TD学习、第7章的n步方法）。\n",
    "  - **规划（Planning）**：利用环境模型（model），通过计算或模拟来改进策略（如第4章的动态规划）。\n",
    "\n",
    "- **挑战**：\n",
    "  - 无模型方法（如Q-learning）直接从经验学习，效率可能较低，尤其在稀疏奖励环境中。\n",
    "  - 基于模型的方法（如动态规划）需要完整模型，但现实中模型通常未知。\n",
    "\n",
    "- **目标**：\n",
    "  - 结合学习和规划的优势：***用经验学习模型，再用模型进行规划***。\n",
    "\n",
    "### 8.2 环境模型（Models）\n",
    "- **定义**：\n",
    "  - 环境模型是对状态转移和奖励的描述，通常包括：\n",
    "    - 状态转移函数：$P(s'|s, a)$（给定状态和动作，下一个状态的概率分布）。\n",
    "    - 奖励函数：$R(s, a, s')$（转移到新状态时的期望奖励）。\n",
    "\n",
    "- **类型**：\n",
    "  - **分布模型（Distribution Models）**：给出完整的概率分布，如马尔可夫决策过程（MDP）的转移概率。\n",
    "  - **采样模型（Sample Models）**：通过模拟生成状态和奖励的样本（如模拟器）。\n",
    "\n",
    "- **作用**：\n",
    "  - 模型允许智能体\"想象\"未来的状态和奖励，而无需真实交互。\n",
    "\n",
    "### 8.3 规划与动态规划的联系\n",
    "- **动态规划回顾**（第4章）：\n",
    "  - 价值迭代：$V(s) \\leftarrow \\max_a \\sum_{s'} P(s'|s, a) [R(s, a, s') + \\gamma V(s')]$\n",
    "  - 策略迭代：评估策略后改进策略。\n",
    "\n",
    "- **规划的本质**：\n",
    "  - 使用模型进行价值更新或策略优化，与动态规划类似，但可以基于部分经验或模拟数据。\n",
    "\n",
    "- **局限**：\n",
    "  - 动态规划假设模型已知，而第8章关注如何从经验中学习模型并用于规划。\n",
    "\n",
    "### 8.4 集成规划与学习的Dyna架构\n",
    "- **Dyna概念**：\n",
    "  - Dyna（Dynamic Programming + Learning）是一种将直接学习（real experience）和规划（simulated experience）结合的框架。\n",
    "  - 核心思想：智能体不仅从真实经验中学习，还通过模型生成模拟经验进行额外更新。\n",
    "\n",
    "- **Dyna-Q算法**：\n",
    "  - Dyna-Q结合Q-learning（无模型学习）和模型-based规划。\n",
    "  - **步骤**：\n",
    "    1. **真实经验**：执行动作 $a_t$，观察 $s_{t+1}$ 和 $r_{t+1}$，更新Q值：\n",
    "       $$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]$$\n",
    "    2. **模型学习**：记录转移 $(s_t, a_t) \\to (s_{t+1}, r_{t+1})$ 到表格模型中。\n",
    "    3. **规划**：从已访问的状态-动作对中随机采样 $(s, a)$，用模型生成 $(s', r)$，更新：\n",
    "       $$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "    - 规划步骤重复n次（n为规划步数）。\n",
    "\n",
    "- **伪代码**：\n",
    "  ```\n",
    "  初始化 Q(s, a) 和 Model(s, a) 为任意值\n",
    "  重复：\n",
    "      s ← 当前状态\n",
    "      选择动作 a（基于Q，如ε-贪婪）\n",
    "      执行 a，观察 r 和 s'\n",
    "      Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') - Q(s, a)]  # 真实经验更新\n",
    "      Model(s, a) ← (s', r)  # 更新模型\n",
    "      重复 n 次（规划）：\n",
    "          从已访问的 (s, a) 中随机采样\n",
    "          从 Model(s, a) 获取 s' 和 r\n",
    "          Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') - Q(s, a)]\n",
    "      s ← s'\n",
    "  ```\n",
    "\n",
    "- **优点**：\n",
    "  - 比纯Q-learning更快收敛，因为规划利用了模型生成的额外经验。\n",
    "  - 比纯动态规划更实际，因为模型是从经验中学习的。\n",
    "\n",
    "### 8.5 Dyna-Q的示例\n",
    "- **迷宫任务**：\n",
    "  - 书中用一个简单迷宫展示了Dyna-Q的效率。\n",
    "  - Q-learning只依赖真实步数找到目标，而Dyna-Q通过规划提前\"预演\"路径，所需真实交互更少。\n",
    "\n",
    "### 8.6 规划中的优先级（Prioritized Sweeping）\n",
    "- **问题**：\n",
    "  - Dyna-Q随机采样状态-动作对进行规划，效率可能不高。\n",
    "\n",
    "- **优先级扫描（Prioritized Sweeping）**：\n",
    "  - 根据更新的大小（即TD误差 $|r + \\gamma \\max_a Q(s', a) - Q(s, a)|$）给状态-动作对分配优先级。\n",
    "  - 优先更新可能带来更大价值变化的状态。\n",
    "\n",
    "- **伪代码片段**：\n",
    "  ```\n",
    "  维护优先级队列 PQueue\n",
    "  每次真实经验后：\n",
    "      计算TD误差，加入队列\n",
    "      重复n次：\n",
    "          从队列中取出优先级最高的 (s, a)\n",
    "          用模型更新 Q(s, a)\n",
    "          检查受影响的状态，更新队列\n",
    "  ```\n",
    "\n",
    "- **效果**：\n",
    "  - 在稀疏奖励任务中显著提高效率。\n",
    "\n",
    "### 8.7 实时动态规划（Real-Time Dynamic Programming, RTDP）\n",
    "- **定义**：\n",
    "  - RTDP是一种基于真实轨迹的规划方法，仅更新智能体实际访问的状态。\n",
    "  - 更新规则与值迭代类似，但只在当前状态上执行。\n",
    "\n",
    "- **特点**：\n",
    "  - 结合了在线学习和规划，适用于实时任务。\n",
    "\n",
    "### 8.8 与前几章的联系\n",
    "- **第6章（TD学习）**：\n",
    "  - TD和Q-learning是无模型方法，Dyna通过引入模型增强了这些算法。\n",
    "\n",
    "- **第7章（n步自举）**：\n",
    "  - n步方法关注多步回报，而Dyna关注如何利用模型模拟多步经验。\n",
    "\n",
    "- **第4章（动态规划）**：\n",
    "  - Dyna将动态规划的思想（模型-based更新）引入经验驱动的学习。\n",
    "\n",
    "### 8.9 理论与实践\n",
    "- **收敛性**：\n",
    "  - 在适当条件下（如充分探索），Dyna-Q和优先级扫描收敛到最优策略。\n",
    "\n",
    "- **局限**：\n",
    "  - 表格方法假设状态和动作空间有限，下一章（第9章）将探讨函数逼近解决大规模问题。\n",
    "\n",
    "- **应用**：\n",
    "  - Dyna架构启发了现代模型基强化学习（如MuZero）。\n",
    "\n",
    "## 总结\n",
    "第8章通过Dyna架构整合了规划与学习，展示了如何用经验构建模型并利用模拟经验加速学习。Dyna-Q和优先级扫描等方法在表格设置下提高了效率，为后续基于模型的强化学习（如第9章的函数逼近）奠定了基础。这一章的核心思想在现代RL（如AlphaGo的蒙特卡洛树搜索）中仍有深远影响。\n"
   ],
   "id": "456822271bba009"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T03:27:59.232059Z",
     "start_time": "2025-04-01T03:27:59.144621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "以下是一个简化的Dyna-Q实现（伪迷宫环境），其中Q值逐渐反映从起点到目标的最优路径。\n",
    "实现了实际交互、模型更新和规划：\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# 简单迷宫：3x3网格，起点(0,0)，目标(2,2)，障碍(1,1)\n",
    "n_rows, n_cols = 3, 3\n",
    "n_states = n_rows * n_cols\n",
    "n_actions = 4  # 上、右、下、左\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "model = {}  # (s, a) -> (s', r)\n",
    "alpha, gamma, epsilon = 0.1, 0.9, 0.1\n",
    "n_planning = 5\n",
    "\n",
    "def state_to_coords(s): return divmod(s, n_cols)\n",
    "def coords_to_state(r, c): return r * n_cols + c\n",
    "def step(s, a):\n",
    "    r, c = state_to_coords(s)\n",
    "    new_r, new_c = r, c\n",
    "    if a == 0: new_r = max(0, r-1)  # 上\n",
    "    elif a == 1: new_c = min(n_cols-1, c+1)  # 右\n",
    "    elif a == 2: new_r = min(n_rows-1, r+1)  # 下\n",
    "    elif a == 3: new_c = max(0, c-1)  # 左\n",
    "\n",
    "    # 检查是否撞到障碍物\n",
    "    if new_r == 1 and new_c == 1:  # 障碍物位置(1,1)\n",
    "        new_r, new_c = r, c  # 撞到障碍物后不移动\n",
    "\n",
    "    s_next = coords_to_state(new_r, new_c)\n",
    "    reward = 1 if s_next == 8 else 0  # 目标(2,2)\n",
    "    return s_next, reward\n",
    "\n",
    "# Dyna-Q训练\n",
    "for episode in range(100):\n",
    "    s = 0  # 起点(0,0)\n",
    "    while s != 8:  # 直到到达目标\n",
    "        # 选择动作\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(Q[s])\n",
    "        # 真实经验\n",
    "        s_next, r = step(s, a)\n",
    "        Q[s, a] += alpha * (r + gamma * np.max(Q[s_next]) - Q[s, a])\n",
    "        model[(s, a)] = (s_next, r)\n",
    "        # 规划\n",
    "        for _ in range(n_planning):\n",
    "            s_sim, a_sim = list(model.keys())[np.random.randint(len(model))]\n",
    "            s_next_sim, r_sim = model[(s_sim, a_sim)]\n",
    "            Q[s_sim, a_sim] += alpha * (r_sim + gamma * np.max(Q[s_next_sim]) - Q[s_sim, a_sim])\n",
    "        s = s_next\n",
    "print(\"Q-table:\\n\", Q)"
   ],
   "id": "a6b0cfedaa3f5fa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      " [[0.65608026 0.7289999  0.59031339 0.65605928]\n",
      " [0.7289933  0.80999999 0.72898023 0.65607906]\n",
      " [0.80999266 0.80997643 0.9        0.72898351]\n",
      " [0.65607076 0.         0.         0.59021391]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.80999529 0.89999495 1.         0.89999092]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T03:28:01.074707Z",
     "start_time": "2025-04-01T03:28:01.070346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "# 可视化Q值\n",
    "表格的每一行代表一个状态（3x3网格共9个状态）。\n",
    "每一列代表一个动作（0上、1右、2下、3左）。Q值越高，表示该状态-动作对越优。\n",
    "换句话说，在每一个状态下（每一行），选择列索引最大的动作（Q值最大的动作）就是最优策略。\n",
    "→ → ↓\n",
    "↑ ↑ ↓\n",
    "↑ ↑ ↑\n",
    "\"\"\"\n",
    "print(\"Optimal policy（最佳策略）:\\n\", np.argmax(Q, axis=1).reshape(n_rows, n_cols))"
   ],
   "id": "9e4d630e415bfe22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy（最佳策略）:\n",
      " [[1 1 2]\n",
      " [0 0 2]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7919668188e609e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
