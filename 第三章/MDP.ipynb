{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 第三章 MDP：Finite Markov Decision Process（有限马尔可夫决策过程）\n",
    "\n",
    "所谓马尔可复决策过程（__MDP__）是指一个五元组$(S, A, P, R, \\gamma)$，其中：\n",
    "\n",
    "- $S$是状态集合\n",
    "- $A$是动作集合\n",
    "- $P$是状态转移概率矩阵\n",
    "- $R$是奖励函数\n",
    "- $\\gamma$是折扣因子\n",
    "\n",
    "_（有限马尔可夫中的 __“有限”__ 表示上述状态集合和动作集合都是有限的，而非无限情况下的。）_\n",
    "\n",
    "MDP中，动作不仅影响当前的即时收益，还影响后续的情境（又称状态）以及未来的收益。因此, MDP 涉及了 __延迟收益__ ，由此也就有了在当前收益和延迟收益之间权衡的需求。在上一章赌博机问题中，我们估计了每个动作 ${a}$ 的价值 $Q(a)$; 而在 MDP 中，我们估计了：\n",
    "- 每个动作 ${a}$ 在每个状态 ${s}$ 中的价值 $Q(s, a)$, 或者\n",
    "- 估计给定最优动作下的每个状态的价值 $V(s)$。 \n",
    "\n",
    "并且这些价值都是长期累积的。\n",
    "\n",
    "MDP是一个非常重要的强化学习模型，它描述了一个智能体与环境交互的过程。智能体在某个状态$s$下执行动作$a$，环境根据状态转移概率$P$将智能体的状态从$s$转移到$s'$，并给予智能体奖励$R$。智能体的目标是最大化长期累积奖励，即最大化折扣累积奖励$\\sum_{t=0}^{\\infty} \\gamma^t r_t$。\n",
    "\n"
   ],
   "id": "4baa16586dde9d25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.1 The Agent-environment Interface（智能体-环境接口）\n",
    "\n",
    "MDP 就是一种通过交互式学习来实现目标的理论框架。那么引出：\n",
    "\n",
    "- 什么是智能体 agent\n",
    "- 什么是环境 environment\n",
    "\n",
    "智能体选择动作，环境对这些动作做出相应的响应，并向智能体呈现出新的状态。环境也会产生一个收益，通常是特定的数值，表示智能体在这个状态下的表现。智能体的目标是最大化这个收益。如下图所示：\n",
    "\n",
    "![MDP](img_1.png)\n",
    "\n",
    "__MDP的公式化定义如下：__\n",
    "\n",
    "$$\n",
    "p(s',r \\mid s,a) \\doteq \\Pr\\{S_t = s', R_t = r \\mid S_{t-i} = s, A_{t-i} = a\\}\n",
    "$$\n",
    "\n",
    "这里，$p(s',r \\mid s,a)$是 __状态转移概率__ 动态函数，表示在状态$s$下执行动作$a$后，智能体将会转移到状态$s'$并获得奖励$r$的概率。\n",
    "\n",
    "\n",
    "（动态函数 $p: S \\times A \\times R \\times S \\rightarrow [0, 1]$ 来自行为的序列或轨迹：$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\ldots$ ）\n",
    "\n",
    "\n",
    "函数 p 为每个 s 和 a 的选择都指定了一个概率分布，即：\n",
    "\n",
    "$$\n",
    "\\sum_{s' \\in S} \\sum_{r \\in R} p(s',r \\mid s,a) = 1, \\quad \\text{对于所有 } s \\in S, a \\in A(s)\n",
    "$$\n",
    "\n",
    "\n",
    "也就是说，$S_t$ 和 $R_t$ 的每个可能的值出现的概率只取决于前一个状态 $S_{t一1}$ 和前一个动作 $A_{t一1}$, 并且与更早之前的状态和动作完全无关。这个限制是针对状态的。状态必须包括过去智能体和环境交互的方方面面的信息，这些信息会对未来产生一定影响。这样，状态就被认为具有 __马尔可夫性__ 。\n",
    "\n",
    "__MDP框架的限制：__\n",
    "\n",
    "_非静态环境_：MDP假设环境和奖励函数是静态的，即它们不会随着时间变化。如果环境是非静态的，MDP框架可能无法充分表示任务。在这些情况下，自适应或在线学习技术可能更合适。\n",
    "\n",
    "__例子：Recycling Robot 扫地机器人：__\n",
    "\n",
    "状态、动作、下一状态、下一状态转移概率、奖励函数等信息整理成表格，如下图所示：\n",
    "\n",
    "（左侧）表格归纳图、（右侧）转移归纳图\n",
    "\n",
    "![Recycling Robot](img.png)"
   ],
   "id": "e6320d3f56a30ad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Goals and Rewards（目标和奖励）\n",
    "\n",
    "- 在强化学习中，智能体的目标是最大化长期累积奖励（即最大化其收到的总收益）。\n",
    "- 如何设计奖励值？胜利+1、失败-1？胜利+100，每一步-1？等等。\n",
    "- 设计最终目标的重要性。赢棋？通关？击败对手？等等。"
   ],
   "id": "cb02bf29f5bda2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.3 Returns and Episodes（回报和回合）",
   "id": "20991b9d12c6454e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.4 Unified Notation for Episodic and Continuing Tasks（回合任务和连续任务的统一符号表示）",
   "id": "8c33c789bc6f76b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.5 Policies and Value Functions（策略和价值函数）",
   "id": "e72760bc172c17d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.6 Optimal Policies and Optimal Value Functions（最优策略和最优价值函数）",
   "id": "1ca7a6bc2a47fd3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.7 Optimality and Approximation（最优性和近似）",
   "id": "17e74c16d07dd02b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.8 Summary（小结）",
   "id": "86e4d4306ca3a17a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "32393db1513c6844"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
