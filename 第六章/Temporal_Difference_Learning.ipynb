{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6630f5c4dcd71397"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "动态规划（Dynamic Programming, __DP__）、蒙特卡洛方法（Monte Carlo, __MC__）和时序差分方法（Temporal-Difference, __TD__）是第4、5、6三章中引出的三种主要的强化学习方法。\n",
    "\n",
    "#### 1. 动态规划（DP）\n",
    "\n",
    " _特点：_\n",
    "\n",
    "- DP方法依赖于完整的环境模型，即要求知道环境的状态转移概率和奖励函数。\n",
    "- DP使用贝尔曼方程，通过迭代更新状态值来计算最优策略。\n",
    "- 适用于具有有限状态和动作空间的确定性问题或具有已知动态模型的情况。\n",
    "\n",
    "_优势：_\n",
    "\n",
    "- 适合有明确模型的环境，能够在已知的状态转移规则下有效计算最优策略。\n",
    "- 收敛性强，在每次迭代中逐渐逼近最优值或策略。\n",
    "\n",
    "_劣势：_\n",
    "\n",
    "- 依赖于环境模型，无法直接应用于没有模型的情况（即不适合未知环境的在线学习）。\n",
    "- 在大规模状态空间时，计算开销大（“维度灾难”），难以扩展到高维空间。\n",
    "\n",
    "\n",
    "#### 2. 蒙特卡洛方法（MC）\n",
    "\n",
    "_特点：_\n",
    "\n",
    "- MC方法不需要环境的模型，只需要通过采样完整的轨迹（即一条完整的状态-动作-奖励序列）来估计状态值或动作值。\n",
    "- 依赖于采样后的实际经验，适合无模型的强化学习环境。\n",
    "\n",
    "_优势：_\n",
    "\n",
    "- 无模型要求，可以直接从经验中学习，适合用于不明确的或复杂的环境。\n",
    "- 可以在环境不确定或存在噪声的情况下收敛，尤其适用于非马尔可夫性决策过程中。\n",
    "\n",
    "_劣势：_\n",
    "\n",
    "- 必须等到一个完整的回合结束后才能更新值，无法在每个步骤中逐步学习。\n",
    "- 在某些稀疏奖励的环境中，MC可能收敛较慢或存在高方差。\n",
    "\n",
    "\n",
    "#### 3. 时序差分方法（TD）\n",
    "\n",
    "_特点：_\n",
    "\n",
    "- TD方法结合了DP和MC的优势，不需要环境模型，但也可以在每个时间步更新价值函数，而不需要等到回合结束。\n",
    "- 它通过当前经验中的部分反馈更新值函数，因此能够处理在线学习。\n",
    "\n",
    "_优势：_\n",
    "\n",
    "- 适合无模型的环境，并且能够在每一步都进行更新，而不必等到整个回合结束。\n",
    "- 相比MC，TD方法的方差较低，因为它通过一步的预测更新状态值。\n",
    "- 适合持续的任务，即那些没有明确结束点的任务（如某些控制问题）。\n",
    "\n",
    "_劣势：_\n",
    "\n",
    "- 与MC相比，TD方法的偏差更大，因为它依赖于当前的估计值更新，而不是基于实际回报。\n",
    "\n",
    "__DP__、 __MC__ 和 __TD__ 三者的对比：\n",
    "\n",
    "| 方法          | 依赖环境模型 | 学习方式       | 更新时机   | 偏差和方差         | 适用场景                  |\n",
    "|---------------|--------|----------------|------------|--------------------|-----------------------|\n",
    "| 动态规划（DP） | 需要     | 全局迭代更新   | 每个状态   | 无偏差，但需要环境模型 | 适用于有限状态、已知环境的情况       |\n",
    "| 蒙特卡洛（MC） | 不需要    | 回合采样更新   | 回合结束时 | 无偏差，但方差大     | 无模型环境，适合回合制终止性任务（如围棋） |\n",
    "| 时序差分（TD） | 不需要    | 时间步反馈   | 每个时间步 | 有偏差，但方差小     | 无模型环境，适合持续性任务（如机器人）   |\n",
    "\n",
    "#### 个人思考：\n",
    "\n",
    "由于已经把三种主要算法的章节都读完了，终于对于他们讲的内容理解得更深刻一些。\n",
    "\n",
    "其实现代的强化学习算法，往往是这三种方法的结合。比如说，蒙特卡洛方法用于那些任务明确终止的场景，而TD方法适合于持续性任务。可以结合使用这两种方法，尤其是在现代很多场景的学习是多环境或多任务的交互的。\n",
    "\n",
    "而且这三章主要理解他们的逻辑，而当今更多是采用神经网络来拟合价值函数，这样可以更好的处理高维度的问题。而神经网络的结构，也是可以结合这三种方法的特点来设计的。\n",
    "\n",
    "所以，这三种方法的理解，对于理解现代强化学习算法的设计思想，还是很有帮助的。"
   ],
   "id": "e1decee0ccc2cd6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7f638eb2235120b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
