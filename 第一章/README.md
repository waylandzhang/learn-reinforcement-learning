## 第一章部分的代码

### 1. black_jack.ipynb

这是我引入一个RL总体概念的例子。

通过OpenAI早期强化学习环境中实现一个21点的游戏的例子。代码来自于官方文档。

代码实现了一个使用Q学习算法解决Blackjack（21点）游戏的强化学习代理。以下是代码的详细介绍：  

#### 导入库：  

导入了必要的库，如matplotlib、numpy、seaborn、tqdm和gymnasium。

#### 创建环境：  

使用gymnasium库创建了Blackjack游戏环境。

#### 定义BlackjackAgent类：  

该类实现了一个Q学习代理，包含以下方法：

    __init__：初始化代理，包括学习率、epsilon值、折扣因子等。

    get_action：根据epsilon贪婪策略选择动作。

    update：更新Q值。

    decay_epsilon：逐渐减少epsilon值以减少探索。

#### 训练代理：  

设置了超参数，如学习率、epsilon值、训练的回合数等。

创建了一个BlackjackAgent实例。

使用for循环进行训练，每个回合中代理与环境交互并更新Q值。

#### 绘制结果：  

使用matplotlib绘制了训练结果，包括回合奖励、回合长度和训练误差的移动平均值。

#### 可视化策略和状态值：  

定义了create_grids和create_plots函数，用于创建和绘制状态值和策略的网格图。

分别绘制了有可用A（A算作11）和无可用A（A算作1）情况下的状态值和策略图。

#### 关闭环境：  

训练和可视化完成后，关闭环境。

### 2. lunar_lander.ipynb

这是我视频课中引入的另一个展示强化学习训练过程和环境的例子。

它是通过模拟多次月球车落地来训练一个强化学习代理，使其学会在不同情况下正确降落。

这段代码运行后会使用for循环模拟1000次登月过程。在每次循环中：

通过env.action_space.sample()随机选择一个动作。

调用env.step(action)执行动作，获取新的观测值、奖励、终止标志、截断标志和信息。

如果环境终止或截断，则重置环境。

### 3. tic_tac_toe.ipynb

这段代码实现了一个井字棋（Tic-Tac-Toe）游戏环境，并使用Q学习算法训练了一个智能体来玩这个游戏。具体步骤如下：  

#### 导入库：  

导入了numpy和random库，用于数值计算和随机选择。

#### 井字棋环境：

定义了TicTacToe类，包含初始化、重置、获取状态、验证动作、执行动作和检查胜利者的方法。

#### Q学习智能体：  

定义了QLearningAgent类，包含初始化、选择动作和更新Q值的方法。

#### 训练函数：  

定义了train函数，创建环境和智能体，通过多次游戏回合训练智能体，更新Q值。

#### 游戏函数：  

定义了play_game函数，使用训练好的智能体与人类玩家进行游戏。

#### 主函数：

在主函数中调用train函数训练智能体，并调用play_game函数进行游戏。